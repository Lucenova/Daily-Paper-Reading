#### accuracy, precision, recall, f1-score in deep learning

True Positive(TP)：prediction is positive, actual is positive

False Positive(FP):  prediction is positive, actual is negative

True Negative(TN): prediction is negative, actual is negative

False Negative(FN): prediction is negative, actual is positive
$$
accuracy = \frac{TP+TN}{TP+TN+FP+FN} \\
precision = \frac{TP}{TP+FP} \\
recall = \frac{TP}{TP+FN} \\
F-score = \frac{2}{\frac{1}{precision}\frac{1}{recall}} \\
specificity = \frac{TN}{TN+FP} \\
sensitivity = \frac{TP}{TP+FN}
$$
**精确率(precision)**从定义上来说就是衡量预测为正的样本是多少真正的正样本

**召回率(recall)**从定义上来说就是衡量在所有正样本中有多少正样本是被预测正确了 **（召回了多少）**

#### f1-score compare with dice coeffcient

F1 Score = Dice Coefficient

#### AUC-ROC curve

ROC曲线横坐标是FPR(False Positive Rate)**实际为负，预测错误的个数**，纵坐标是TPR(True Positive Rate)**实际为正并且预测正确的个数**。

#### 我们再来看看ROC曲线中的“四点一线”

- 第一个点，(0,1)，即FPR=0, TPR=1，这意味着FN（false negative）=0，并且FP（false positive）=0。这是一个完美的分类器，它将所有的样本都正确分类。
- 第二个点，(1,0)，即FPR=1，TPR=0，类似地分析可以发现这是一个最糟糕的分类器，因为它成功避开了所有的正确答案。
- 第三个点，(0,0)，即FPR=TPR=0，即FP（false positive）=TP（true positive）=0，可以发现该分类器预测所有的样本都为负样本（negative）。
- 第四个点（1,1），分类器实际上预测所有的样本都为正样本。

对于ROC曲线，我们可以这样理解，对于二分类问题，曲线的每一个点都代表一个阈值，分类器给每个样本一个得分，得分大于阈值的我们认为是正样本，小于阈值的我们认为是负样本。

我们随便取一个点比如（0.5，0.7），那么也就说明了这个分类器在FP/N = 0.5的时候，也就是实际值为负，而我预测错了预测成正的数量占整个实际值为负的数量的0.5。这句话看起来很拗口，其实也就是对于负值的预测正确率为1-0.5=0.5，TP/P=0.7,同理，就是对于正值的预测正确率为0.7。所以说，ROC曲线中的每一个点都代表一组测试数据中，对于正值和负值的预测正确率分别的考量

#### ROC曲线图的几个性质

(**性质1**)若学习器A的ROC曲线将另外一个学习器B的曲线完全包住，则A的性能一定比B好，否则若二者曲线有交叉，则可以较为合理的认为ROC曲线越接近左上角，也就是AUC值越大，整个分类器的性能越好。

### AUC值

AUC值 Area Under Curve (曲线下面积)，也就是ROC曲线之下与坐标轴围成的面积。
我们很容易就会看出来，AUC其实是ROC    曲线的积分，但是这样很不直观，也不能感性理解，其实AUC确实是有物理意义的。AUC是指 随机给定一个正样本和一个负样本，分类器输出的正样本的概率比分类器出去负样本的概率大的可能性。

很好玩的感性认识：

https://juejin.cn/post/6844903514012024840
### What matters when building vision-language models?

#### Exploring the design space of vision-language models

- For a fixed number of parameters, the quality of the language model backbone has a higher impact on the performance of the final VLM than the quality of the vision backbone.
- The cross-attention architecture performs better than the fully autoregressive one when unimodal pre-trained backbones are kept frozen. However, when training the unimodal backbones, the fully autogressive architecture outperforms the cross-attention one, even though the latter has more parameters.
- Unfreezing the pre-trained backbones under the fully autoregressive architecture can lead to training divergences. Leveraging LoRA still adds expressivity to the training and stabilizes it.
- Reducing the number of visual tokens with learned pooling significantly improves compute efficiency at training and inference while improving performance on downstream tasks.
- Adapting a vision encoder pre-trained on fixed-size square images to preserve imagesâ€™ original aspect ratio and resolution does not degrade performance while speeding up training and inference and reducing memory.
- Splitting images into sub-images during training allow trading compute efficiency for more performance during inference. The increase in performance is particularly noticeable in tasks involving reading text in an image.

